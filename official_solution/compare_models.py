import os

import torch
from transformers import AutoModel


def load_model(folder_path):
    model = AutoModel.from_pretrained(folder_path)
    return model


def compute_norm_and_max_diff(model1, model2):
    # Ensure models are in evaluation mode
    model1.eval()
    model2.eval()

    # Collect all parameters from both models
    params1 = list(model1.parameters())
    params2 = list(model2.parameters())

    # Ensure both models have the same number of parameters
    assert len(params1) == len(
        params2
    ), "Models do not have the same number of parameters"

    # Compute the difference between parameters and calculate norms
    diff_norm = 0.0
    max_diff = 0.0

    for p1, p2 in zip(params1, params2):
        diff = p1 - p2
        diff_norm += torch.norm(diff).item()
        max_diff = max(max_diff, torch.max(torch.abs(diff)).item())

    return diff_norm, max_diff


# Replace 'path_to_model1' and 'path_to_model2' with your actual model folder paths
model1_folder = "./trained_model_reference"
model2_folder = "./trained_model"
deviation_norm_limit = 3

if not os.path.exists(model1_folder) or not os.path.exists(model2_folder):
    print("Trained model not found")
    exit()
model1 = load_model(model1_folder)
model2 = load_model(model2_folder)

norm_diff, max_scalar_diff = compute_norm_and_max_diff(model1, model2)

print(
    f"Norm of deviation: {norm_diff}, max scalar deviation: {max_scalar_diff}, {'' if norm_diff < deviation_norm_limit else f'norm deviation exceeds limit of {deviation_norm_limit}'}"
)
